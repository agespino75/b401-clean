{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Chains\n",
    "* Perform several actions in a particular order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f",
   "metadata": {},
   "source": [
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2",
   "metadata": {},
   "source": [
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 004-chains.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **004-chains**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746c97-9fa5-481c-8333-21de1504a087",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:05.951971Z",
     "start_time": "2025-06-08T05:19:05.943306Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa888f7-3718-4829-8645-30acb43db51f",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
   "metadata": {},
   "source": [
    "## LLM Model\n",
    "* The trend before the launch of chatGPT-4.\n",
    "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:16.600335Z",
     "start_time": "2025-06-08T05:19:15.910617Z"
    }
   },
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llmModel = OpenAI()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
   "metadata": {},
   "source": [
    "## Chat Model\n",
    "* The general trend after the launch of chatGPT-4.\n",
    "    * Frequently known as \"Chatbot\". \n",
    "    * Conversation between Human and AI.\n",
    "    * Can have a system prompt defining the tone or the role of the AI. \n",
    "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
    "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
   ]
  },
  {
   "cell_type": "code",
   "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:20.401167Z",
     "start_time": "2025-06-08T05:19:20.375723Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c2f91601-8594-41d3-9316-d51791fc54e8",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "* See the LangChain documentation about prompts [here](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/).\n",
    "* Input into LLMs.\n",
    "* Prompt templates: easier to use prompts with variables. A prompt template may include:\n",
    "    * instructions,\n",
    "    * few-shot examples,\n",
    "    * and specific context and questions appropriate for a given task."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:29.383736Z",
     "start_time": "2025-06-08T05:19:26.350917Z"
    }
   },
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}.\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\", \n",
    "    topic=\"the Kennedy family\"\n",
    ")\n",
    "\n",
    "llmModel.invoke(llmModelPrompt)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nIn 1963, just months before President John F. Kennedy was assassinated, a bizarre and mysterious incident occurred involving his brother, Attorney General Robert F. Kennedy.\\n\\nOn a hot summer day in Washington D.C., Robert Kennedy was taking a dip in the pool at his home when he suddenly heard a commotion and screams coming from the Kennedy family\\'s pet orangutan, Charlie. As Robert rushed to the poolside, he found that Charlie had somehow managed to fall into the water and was struggling to stay afloat.\\n\\nWithout hesitation, Robert jumped into the pool and rescued Charlie, pulling him out of the water and onto the pool deck. As he did, he noticed something strange â€“ Charlie was wearing a tiny wetsuit.\\n\\nConfused and amused, the Kennedy family investigated further and discovered that their children\\'s nanny, who often took care of Charlie, had made the wetsuit for the orangutan as a joke. However, no one could explain how Charlie had managed to put on the wetsuit by himself and fall into the pool.\\n\\nThe incident became known as the \"Orangutan Pool Incident\" and became a popular story among the Kennedy family. It was even rumored that President Kennedy himself joked about it, saying that Robert was the only one who could'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:35.725591Z",
     "start_time": "2025-06-08T05:19:34.015248Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:38.879573Z",
     "start_time": "2025-06-08T05:19:38.873198Z"
    }
   },
   "source": [
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 35 grandchildren. He and his wife, Rose Fitzgerald Kennedy, had nine children, and many of those children went on to have multiple children of their own.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 55, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bg2jOibrMIJaVyQrU9J9KFYANbiBy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a36f0f10-a0a4-4eaf-8eeb-019483652f2a-0', usage_metadata={'input_tokens': 55, 'output_tokens': 48, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:42.447535Z",
     "start_time": "2025-06-08T05:19:42.444165Z"
    }
   },
   "source": [
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 35 grandchildren. He and his wife, Rose Fitzgerald Kennedy, had nine children, and many of those children went on to have multiple children of their own.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 55, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bg2jOibrMIJaVyQrU9J9KFYANbiBy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--a36f0f10-a0a4-4eaf-8eeb-019483652f2a-0' usage_metadata={'input_tokens': 55, 'output_tokens': 48, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:19:46.610743Z",
     "start_time": "2025-06-08T05:19:46.607251Z"
    }
   },
   "source": [
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 35 grandchildren. He and his wife, Rose Fitzgerald Kennedy, had nine children, and many of those children went on to have multiple children of their own.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2",
   "metadata": {},
   "source": [
    "#### Old way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6dafd17-47a2-4169-992e-76ffb9702d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are an Historian expert on the Kennedy family.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children:\n",
      "\n",
      "1. Joseph P. Kennedy Jr.\n",
      "2. John F. Kennedy\n",
      "3. Rosemary Kennedy\n",
      "4. Kathleen Kennedy\n",
      "5. Eunice Kennedy\n",
      "6. Patricia Kennedy\n",
      "7. Robert F. Kennedy\n",
      "8. Jean Kennedy\n",
      "9. Edward M. Kennedy\n",
      "\n",
      "Their grandchildren include:\n",
      "\n",
      "- Caroline Kennedy (daughter of John F. Kennedy)\n",
      "- John F. Kennedy Jr. (son of John F. Kennedy)\n",
      "- Patrick J. Kennedy (son of Edward M. Kennedy)\n",
      "- Robert F. Kennedy Jr. (son of Robert F. Kennedy)\n",
      "- Maria Shriver (daughter of Eunice Kennedy)\n",
      "\n",
      "These are just a few examples of the grandchildren of Joseph P. Kennedy.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41",
   "metadata": {},
   "source": [
    "#### What is the full potential of ChatPromptTemplate?\n",
    "* Check the [corresponding page](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) in the LangChain API."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:37:25.779829Z",
     "start_time": "2025-06-08T05:37:25.777025Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_core.prompts import FewShotChatMessagePromptTemplate",
   "id": "223a3181d713bf4c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:51:32.546431Z",
     "start_time": "2025-06-08T05:51:32.538124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"Â¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"Â¡adiÃ³s!\"},\n",
    "    {\"input\": \"how are you?\", \"output\": \"Â¿cÃ³mo estÃ¡s?\"},\n",
    "    {\"input\": \"what's your name?\", \"output\": \"Â¿cuÃ¡l es tu nombre?\"},\n",
    "    {\"input\": \"I love programming.\", \"output\": \"me encanta programar.\"},\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Â¿cuÃ¡l es la capital de Francia?\"},\n",
    "    {\"input\": \"Translate 'good morning' to Spanish.\", \"output\": \"traduce 'buenos dÃ­as' al espaÃ±ol.\"},\n",
    "    {\"input\": \"Translate 'good night' to Spanish.\", \"output\": \"traduce 'buenas noches' al espaÃ±ol.\"},\n",
    "\n",
    "]"
   ],
   "id": "7475f997e8b341fe",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:51:36.443925Z",
     "start_time": "2025-06-08T05:51:35.609550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chatModel\n",
    "\n",
    "response=chain.invoke({\"input\": \"Who was JFK?\"})\n",
    "print(response.content)"
   ],
   "id": "cb4235d858c3b12c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¿QuiÃ©n fue JFK?\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d9f8be09-99fc-491e-babb-0b8ad5bc74c2",
   "metadata": {},
   "source": [
    "## Our first chain: an example of few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "id": "b973d828-0f47-4c43-98ab-e3db306d41e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T05:37:02.843741Z",
     "start_time": "2025-06-08T05:37:02.728636Z"
    }
   },
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chatModel\n",
    "\n",
    "response = chain.invoke({\"input\": \"Who was JFK?\"})\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'output'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     13\u001B[39m final_prompt = ChatPromptTemplate.from_messages(\n\u001B[32m     14\u001B[39m     [\n\u001B[32m     15\u001B[39m         (\u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mYou are an English-Spanish translator.\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     18\u001B[39m     ]\n\u001B[32m     19\u001B[39m )\n\u001B[32m     21\u001B[39m chain = final_prompt | chatModel\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m response = \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWho was JFK?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[38;5;28mprint\u001B[39m(response.content)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/runnables/base.py:3045\u001B[39m, in \u001B[36mRunnableSequence.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3043\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m   3044\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i == \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m3045\u001B[39m         input_ = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3046\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3047\u001B[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/base.py:216\u001B[39m, in \u001B[36mBasePromptTemplate.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tags:\n\u001B[32m    215\u001B[39m     config[\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m] = config[\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m] + \u001B[38;5;28mself\u001B[39m.tags\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_format_prompt_with_error_handling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrun_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[43mserialized\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_serialized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/runnables/base.py:1940\u001B[39m, in \u001B[36mRunnable._call_with_config\u001B[39m\u001B[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001B[39m\n\u001B[32m   1936\u001B[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001B[32m   1937\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(child_config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m   1938\u001B[39m         output = cast(\n\u001B[32m   1939\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mOutput\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m-> \u001B[39m\u001B[32m1940\u001B[39m             \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1941\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   1942\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1943\u001B[39m \u001B[43m                \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1944\u001B[39m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1945\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1946\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1947\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m   1948\u001B[39m         )\n\u001B[32m   1949\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1950\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/runnables/config.py:428\u001B[39m, in \u001B[36mcall_func_with_variable_args\u001B[39m\u001B[34m(func, input, config, run_manager, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[32m    427\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m] = run_manager\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/base.py:190\u001B[39m, in \u001B[36mBasePromptTemplate._format_prompt_with_error_handling\u001B[39m\u001B[34m(self, inner_input)\u001B[39m\n\u001B[32m    188\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_format_prompt_with_error_handling\u001B[39m(\u001B[38;5;28mself\u001B[39m, inner_input: \u001B[38;5;28mdict\u001B[39m) -> PromptValue:\n\u001B[32m    189\u001B[39m     _inner_input = \u001B[38;5;28mself\u001B[39m._validate_input(inner_input)\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43m_inner_input\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/chat.py:726\u001B[39m, in \u001B[36mBaseChatPromptTemplate.format_prompt\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    717\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat_prompt\u001B[39m(\u001B[38;5;28mself\u001B[39m, **kwargs: Any) -> PromptValue:\n\u001B[32m    718\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Format prompt. Should return a PromptValue.\u001B[39;00m\n\u001B[32m    719\u001B[39m \n\u001B[32m    720\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    724\u001B[39m \u001B[33;03m        PromptValue.\u001B[39;00m\n\u001B[32m    725\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m726\u001B[39m     messages = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    727\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatPromptValue(messages=messages)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/chat.py:1187\u001B[39m, in \u001B[36mChatPromptTemplate.format_messages\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m   1183\u001B[39m     result.extend([message_template])\n\u001B[32m   1184\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m   1185\u001B[39m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001B[32m   1186\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1187\u001B[39m     message = \u001B[43mmessage_template\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1188\u001B[39m     result.extend(message)\n\u001B[32m   1189\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:390\u001B[39m, in \u001B[36mFewShotChatMessagePromptTemplate.format_messages\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    388\u001B[39m \u001B[38;5;66;03m# Get the examples to use.\u001B[39;00m\n\u001B[32m    389\u001B[39m examples = \u001B[38;5;28mself\u001B[39m._get_examples(**kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m390\u001B[39m examples = \u001B[43m[\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexample_prompt\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput_variables\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mexamples\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[38;5;66;03m# Format the examples.\u001B[39;00m\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m    395\u001B[39m     message\n\u001B[32m    396\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m examples\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.example_prompt.format_messages(**example)\n\u001B[32m    398\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:391\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    388\u001B[39m \u001B[38;5;66;03m# Get the examples to use.\u001B[39;00m\n\u001B[32m    389\u001B[39m examples = \u001B[38;5;28mself\u001B[39m._get_examples(**kwargs)\n\u001B[32m    390\u001B[39m examples = [\n\u001B[32m--> \u001B[39m\u001B[32m391\u001B[39m     \u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexample_prompt\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput_variables\u001B[49m\u001B[43m}\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m examples\n\u001B[32m    392\u001B[39m ]\n\u001B[32m    393\u001B[39m \u001B[38;5;66;03m# Format the examples.\u001B[39;00m\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m    395\u001B[39m     message\n\u001B[32m    396\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m examples\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.example_prompt.format_messages(**example)\n\u001B[32m    398\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/langchain_core/prompts/few_shot.py:391\u001B[39m, in \u001B[36m<dictcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    388\u001B[39m \u001B[38;5;66;03m# Get the examples to use.\u001B[39;00m\n\u001B[32m    389\u001B[39m examples = \u001B[38;5;28mself\u001B[39m._get_examples(**kwargs)\n\u001B[32m    390\u001B[39m examples = [\n\u001B[32m--> \u001B[39m\u001B[32m391\u001B[39m     {k: \u001B[43me\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.example_prompt.input_variables} \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m examples\n\u001B[32m    392\u001B[39m ]\n\u001B[32m    393\u001B[39m \u001B[38;5;66;03m# Format the examples.\u001B[39;00m\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m    395\u001B[39m     message\n\u001B[32m    396\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m examples\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.example_prompt.format_messages(**example)\n\u001B[32m    398\u001B[39m ]\n",
      "\u001B[31mKeyError\u001B[39m: 'output'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 001-connect-llms.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-chains.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
